{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark context - Spark Session\n",
    "It allows Spark Driver to access the cluster through its Cluster Resource Manager and can be used to create RDDs, accumulators and broadcast variables on the cluster. \n",
    "Spark Context also tracks executors in real-time by sending regular heartbeat messages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = (SparkSession.builder \n",
    "        .master(\"local\") \n",
    "        .appName(\"workshop_spark\")\n",
    "        #.option(\"spark.driver.bindAddress\",\"localhost:4040\")\n",
    "        .getOrCreate())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://c31032a256ec:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.4.4</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>workshop_spark</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f3421598400>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "municipios = spark.read.option(\"header\", True).csv(\"data/municipios.csv\")\n",
    "populacao = spark.read.option(\"header\", True).csv(\"data/populacao_municipios.csv\")\n",
    "nomes = spark.read.option(\"header\", True).csv(\"data/nomes_municipios.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+--------------+----------------+---------------+----------------+------------------+--------------+----------+---------------+--------------+------------------+---------------+-----------------------+--------------------+---------------+-------------+--------------+-----------------+---------+------------+--------+------+------------+------------+------------+\n",
      "|id_municipio|id_municipio_6|id_municipio_TSE|id_municipio_RF|id_municipio_BCB|         municipio|capital_estado|id_comarca|id_regiao_saude|  regiao_saude|id_regiao_imediata|regiao_imediata|id_regiao_intermediaria|regiao_intermediaria|id_microrregiao| microrregiao|id_mesorregiao|      mesorregiao|id_estado|estado_abrev|  estado|regiao|existia_1991|existia_2000|existia_2010|\n",
      "+------------+--------------+----------------+---------------+----------------+------------------+--------------+----------+---------------+--------------+------------------+---------------+-----------------------+--------------------+---------------+-------------+--------------+-----------------+---------+------------+--------+------+------------+------------+------------+\n",
      "|     1100023|        110002|              78|              7|            9393|         Ariquemes|             0|   1100023|          11001|Vale do Jamari|            110002|      Ariquemes|                   1101|         Porto Velho|          11003|    Ariquemes|          1102|Leste Rondoniense|       11|          RO|Rondônia| Norte|           1|           1|           1|\n",
      "|     1100106|        110010|              19|              1|           22882|     Guajará-Mirim|             0|   1100106|          11004|Madeira-Mamoré|            110001|    Porto Velho|                   1101|         Porto Velho|          11002|Guajará-Mirim|          1101|  Madeira-Guaporé|       11|          RO|Rondônia| Norte|           1|           1|           1|\n",
      "|     1100114|        110011|             159|             15|           41258|              Jaru|             0|   1100114|          11003|       Central|            110003|           Jaru|                   1101|         Porto Velho|          11004|    Ji-Paraná|          1102|Leste Rondoniense|       11|          RO|Rondônia| Norte|           1|           1|           1|\n",
      "|     1100130|        110013|             396|             39|           44547|Machadinho D'Oeste|             0|   1100122|          11001|Vale do Jamari|            110003|           Jaru|                   1101|         Porto Velho|          11003|    Ariquemes|          1102|Leste Rondoniense|       11|          RO|Rondônia| Norte|           1|           1|           1|\n",
      "|     1100205|        110020|              35|              3|           30719|       Porto Velho|             1|   1100205|          11004|Madeira-Mamoré|            110001|    Porto Velho|                   1101|         Porto Velho|          11001|  Porto Velho|          1101|  Madeira-Guaporé|       11|          RO|Rondônia| Norte|           1|           1|           1|\n",
      "+------------+--------------+----------------+---------------+----------------+------------------+--------------+----------+---------------+--------------+------------------+---------------+-----------------------+--------------------+---------------+-------------+--------------+-----------------+---------+------------+--------+------+------------+------------+------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "municipios.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['id_municipio',\n",
       " 'id_municipio_6',\n",
       " 'id_municipio_TSE',\n",
       " 'id_municipio_RF',\n",
       " 'id_municipio_BCB',\n",
       " 'municipio',\n",
       " 'capital_estado',\n",
       " 'id_comarca',\n",
       " 'id_regiao_saude',\n",
       " 'regiao_saude',\n",
       " 'id_regiao_imediata',\n",
       " 'regiao_imediata',\n",
       " 'id_regiao_intermediaria',\n",
       " 'regiao_intermediaria',\n",
       " 'id_microrregiao',\n",
       " 'microrregiao',\n",
       " 'id_mesorregiao',\n",
       " 'mesorregiao',\n",
       " 'id_estado',\n",
       " 'estado_abrev',\n",
       " 'estado',\n",
       " 'regiao',\n",
       " 'existia_1991',\n",
       " 'existia_2000',\n",
       " 'existia_2010']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "municipios.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-usar partition (pq fazer)\n",
    "-fazer caching (reduz o custo de io)\n",
    "-join\n",
    "-mostrar o explain()?\n",
    "-usar udf (quais os perigos) - Use the higher-level standard Column-based functions with\n",
    "Dataset operators whenever possible before reverting to\n",
    "using your own custom UDF functions since UDFs are a\n",
    "blackbox for Spark and so it does not even try to optimize them.\n",
    "-window?\n",
    "-escrever parquet com particionamento\n",
    "\n",
    "-no final rodar em nuvem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Particionamento\n",
    "https://luminousmen.com/post/spark-partitions\n",
    "https://luminousmen.com/post/spark-tips-partition-tuning\n",
    "\n",
    "- Particionamento é a principal unidade de paralelismo no Apache Spark.\n",
    "- Cada partição é enviada para workers\n",
    "- artições de mais ou de menos podem afetar performance:\n",
    "    - Partição de menos: Não utiliza bem os recursos do cluster\n",
    "    - Partições de mais: Introduz overhead no gerenciamento de muitas partições\n",
    "\n",
    "-----\n",
    "\n",
    "And manual partitioning is important for proper data balancing between partitions. As a rule, smaller partitions allow us to distribute data more evenly, among more executors. Therefore, smaller partitions can boost tasks with more repartitions (data reorganization during computation).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Particionamento padrão: 1\n",
      "Particionamento \"aleatório\": 10\n",
      "Particionamento com chave de Negócio: 200\n"
     ]
    }
   ],
   "source": [
    "print(f'Particionamento padrão: {municipios.rdd.getNumPartitions()}')\n",
    "repartition = municipios.repartition(10)\n",
    "print(f'Particionamento \"aleatório\": {repartition.rdd.getNumPartitions()}')\n",
    "repartition = municipios.repartition('id_municipio')\n",
    "print(f'Particionamento com chave de Negócio: {repartition.rdd.getNumPartitions()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Salvar parquet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cache\n",
    "https://towardsdatascience.com/apache-spark-caching-603154173c48\n",
    "\n",
    "Caching is beneficial when we use particular RDD several times (and can slow down our calculations otherwise where the whole lineage graph will be processed several times).\n",
    "Como funciona? O dataset fica em memória em todos os workers\n",
    "\n",
    "---> broadcast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "populacao.cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#filtrar populacao para 2010\n",
    "#filtrar municipios para os que existiam em 2010\n",
    "\n",
    "#Join\n",
    "df = (nomes.join(municipios, \"id_municipio\")\n",
    "          .join(populacao, \"id_municipio\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## UDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def squared(s):\n",
    "    return s + s\n",
    "squared2 = spark.udf.register(\"squaredWithPython\", squared, LongType())\n",
    "df_sp.withColumn(\"square\", squared2(col(\"Confirmed\").cast(IntegerType()))).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Windows\n",
    "\n",
    "-----\n",
    "-Fazer janela de dado temporal para pegar o lag() como coluna de valor anterior\n",
    "-Pegar valor máximo dos últimos 10 anos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Execution plan\n",
    "- Transform: operações de transformações dos dados do RDD, que resultam em um novo RDD.lazy que não são executadas naquele momento\n",
    "    - narrow: não precisam de dados de outras partições (map)\n",
    "    - wide: precisam de shuffle (groupby)\n",
    "- Ações: operações que precisam avaliar o dataframe como um todo (show(), write()). É quando ele executa o plano\n",
    "\n",
    "--------\n",
    "chamar explain, fazer operação, chamar explain, fazer show(), chamar explain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.explain(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checkpoint\n",
    "https://medium.com/@adrianchang/apache-spark-checkpointing-ebd2ec065371"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Referências\n",
    "\n",
    "- Série inicial: https://luminousmen.com/post/hadoop-yarn-spark\n",
    "- Série 2: https://luminousmen.com/post/spark-tips-dataframe-api\n",
    "- https://www.youtube.com/watch?v=daXEp4HmS-E&feature=youtu.be"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
